{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dbbe65c",
   "metadata": {},
   "source": [
    "## Customer Segmentation Notebook\n",
    "\n",
    "This single-file, runnable Python notebook/script walks you through:\n",
    "1. Loading dataset (Mall Customers or UCI Online Retail II)\n",
    "2. EDA (basic stats & missingness)\n",
    "3. RFM feature engineering (for transactional data)\n",
    "4. Preprocessing & scaling\n",
    "5. Clustering: KMeans (with elbow/silhouette helper) and DBSCAN\n",
    "6. Dimensionality reduction for visualization (PCA + optional UMAP)\n",
    "7. Cluster profiling and export (CSV) ready for Power BI\n",
    "8. A simple ETL function to re-run RFM & assign clusters (for scheduling)\n",
    "\n",
    "Instructions:\n",
    "- Put your dataset CSV in `data/` folder. Filenames expected by default:\n",
    "    - `data/mall_customers.csv`  (Mall Customers small dataset)\n",
    "    - OR `data/online_retail.csv` (UCI Online Retail / Online Retail II export)\n",
    "- Optional: create a virtualenv/conda and `pip install -r requirements.txt` with\n",
    "    pandas, numpy, scikit-learn, matplotlib, seaborn, umap-learn (optional), joblib\n",
    "\n",
    "Run this script cell-by-cell in Jupyter or as a .py script. The script tries to be robust to small format differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5961b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required packages\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score\n",
    "import joblib\n",
    "\n",
    "try:\n",
    "    import umap\n",
    "    _HAS_UMAP = True\n",
    "except Exception:\n",
    "    _HAS_UMAP = False\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3146729d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# load data\n",
    "# -----------------------------\n",
    "\n",
    "def load_dataset(preferred='mall'):\n",
    "    \"\"\"Try to load mall_customers.csv or online_retail.csv from data/ directory.\n",
    "    preferred: 'mall' or 'retail'\n",
    "    Returns (df, dataset_name)\n",
    "    \"\"\"\n",
    "    base = 'data'\n",
    "    mall_path = os.path.join(base, 'mall_customers.csv')\n",
    "    retail_path = os.path.join(base, 'online_retail.csv')\n",
    "\n",
    "    if preferred == 'mall' and os.path.exists(mall_path):\n",
    "        df = pd.read_csv(mall_path)\n",
    "        return df, 'mall'\n",
    "    if preferred == 'retail' and os.path.exists(retail_path):\n",
    "        df = pd.read_csv(retail_path)\n",
    "        return df, 'retail'\n",
    "\n",
    "    # fallback\n",
    "    if os.path.exists(mall_path):\n",
    "        return pd.read_csv(mall_path), 'mall'\n",
    "    if os.path.exists(retail_path):\n",
    "        return pd.read_csv(retail_path), 'retail'\n",
    "\n",
    "    raise FileNotFoundError('No dataset found. Put mall_customers.csv or online_retail.csv in the data/ folder.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0909441a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Basic EDA function\n",
    "# -----------------------------\n",
    "\n",
    "def basic_eda(df, max_rows=5):\n",
    "    print('Shape:', df.shape)\n",
    "    print('\\nColumns and dtypes:')\n",
    "    print(df.dtypes)\n",
    "    print('\\nNull counts:')\n",
    "    print(df.isnull().sum().sort_values(ascending=False).head(20))\n",
    "    print('\\nSample rows:')\n",
    "    display = df.head(max_rows)\n",
    "    print(display)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908d3ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# RFM computation (for transactional retail)\n",
    "# -----------------------------\n",
    "\n",
    "def compute_rfm_from_retail(df, customer_id_col='CustomerID', invoice_date_col='InvoiceDate', invoice_no_col='InvoiceNo', amount_col='TotalAmount', snapshot_date=None):\n",
    "    \"\"\"\n",
    "    Expects a transactional dataframe with one transaction per row.\n",
    "    If your dataset does not have a TotalAmount column, create it as Quantity * UnitPrice.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    # Standardize columns if necessary\n",
    "    if invoice_date_col in df.columns and not np.issubdtype(df[invoice_date_col].dtype, np.datetime64):\n",
    "        df[invoice_date_col] = pd.to_datetime(df[invoice_date_col], errors='coerce')\n",
    "\n",
    "    if amount_col not in df.columns and 'Quantity' in df.columns and 'UnitPrice' in df.columns:\n",
    "        df[amount_col] = df['Quantity'] * df['UnitPrice']\n",
    "\n",
    "    if snapshot_date is None:\n",
    "        snapshot_date = df[invoice_date_col].max() + pd.Timedelta(days=1)\n",
    "\n",
    "    # Filter out canceled rows if InvoiceNo has a 'C' (common in Online Retail)\n",
    "    if invoice_no_col in df.columns:\n",
    "        if df[invoice_no_col].dtype == object:\n",
    "            df = df[~df[invoice_no_col].astype(str).str.contains('C', na=False)]\n",
    "\n",
    "    # Drop rows missing customer id\n",
    "    df = df.dropna(subset=[customer_id_col])\n",
    "\n",
    "    # Aggregate R, F, M\n",
    "    rfm = df.groupby(customer_id_col).agg(\n",
    "        Recency=('InvoiceDate', lambda x: (snapshot_date - x.max()).days),\n",
    "        Frequency=(invoice_no_col, 'nunique'),\n",
    "        Monetary=(amount_col, 'sum')\n",
    "    ).reset_index()\n",
    "\n",
    "    # Handle possible zeros or negatives\n",
    "    rfm['Monetary'] = rfm['Monetary'].clip(lower=0)\n",
    "    return rfm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adc0d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# For Mall Customers dataset: minimal features\n",
    "# -----------------------------\n",
    "\n",
    "def preprocess_mall(df):\n",
    "    # Expected columns: CustomerID (optional), Age, Annual Income (k$), Spending Score (1-100), Gender\n",
    "    df = df.copy()\n",
    "    # Rename common variants\n",
    "    cols = {c: c.strip() for c in df.columns}\n",
    "    df.columns = list(cols.keys())\n",
    "\n",
    "    # If CustomerID missing, create one\n",
    "    if 'CustomerID' not in df.columns:\n",
    "        df['CustomerID'] = range(1, len(df)+1)\n",
    "\n",
    "    # Select a sane subset\n",
    "    features = []\n",
    "    for cand in ['Age', 'Annual Income (k$)', 'Annual Income', 'Spending Score (1-100)', 'Spending Score']:\n",
    "        if cand in df.columns:\n",
    "            features.append(cand)\n",
    "    if 'Gender' in df.columns:\n",
    "        features.append('Gender')\n",
    "\n",
    "    if not features:\n",
    "        # fallback: use numeric columns\n",
    "        features = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "    proc = df[['CustomerID'] + features].copy()\n",
    "    # Basic cleaning\n",
    "    proc = proc.dropna(subset=['CustomerID'])\n",
    "    return proc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f098a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Scaling & clustering helpers\n",
    "# -----------------------------\n",
    "\n",
    "def scale_features(X, scaler=None):\n",
    "    if scaler is None:\n",
    "        scaler = StandardScaler()\n",
    "        Xs = scaler.fit_transform(X)\n",
    "        return Xs, scaler\n",
    "    else:\n",
    "        return scaler.transform(X), scaler\n",
    "\n",
    "\n",
    "def find_best_k(X, k_min=2, k_max=10):\n",
    "    \"\"\"Compute inertia (elbow) and silhouette for k in range.\"\"\"\n",
    "    results = []\n",
    "    for k in range(k_min, k_max+1):\n",
    "        km = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "        labels = km.fit_predict(X)\n",
    "        sil = silhouette_score(X, labels)\n",
    "        results.append({'k':k, 'inertia': km.inertia_, 'silhouette': sil})\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "def run_kmeans(X, k):\n",
    "    km = KMeans(n_clusters=k, random_state=42, n_init=20)\n",
    "    labels = km.fit_predict(X)\n",
    "    return km, labels\n",
    "\n",
    "\n",
    "def run_dbscan(X, eps=0.5, min_samples=5):\n",
    "    db = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    labels = db.fit_predict(X)\n",
    "    return db, labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e075ff43",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Visualization helpers\n",
    "# -----------------------------\n",
    "\n",
    "def plot_elbow_silhouette(df_res):\n",
    "    fig, ax1 = plt.subplots(1,2, figsize=(12,4))\n",
    "    ax1[0].plot(df_res['k'], df_res['inertia'], marker='o')\n",
    "    ax1[0].set_xlabel('k'); ax1[0].set_title('Inertia (Elbow)')\n",
    "    ax1[1].plot(df_res['k'], df_res['silhouette'], marker='o')\n",
    "    ax1[1].set_xlabel('k'); ax1[1].set_title('Silhouette Score')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_clusters_2d(X, labels, title='Clusters (2D PCA)'):\n",
    "    pca = PCA(n_components=2, random_state=42)\n",
    "    comp = pca.fit_transform(X)\n",
    "    dfp = pd.DataFrame(comp, columns=['PC1','PC2'])\n",
    "    dfp['label'] = labels\n",
    "    plt.figure(figsize=(8,6))\n",
    "    sns.scatterplot(data=dfp, x='PC1', y='PC2', hue='label', palette='tab10', s=40)\n",
    "    plt.title(title)\n",
    "    plt.legend(bbox_to_anchor=(1.05,1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def umap_plot(X, labels):\n",
    "    if not _HAS_UMAP:\n",
    "        print('UMAP not installed. Install with: pip install umap-learn')\n",
    "        return\n",
    "    reducer = umap.UMAP(random_state=42)\n",
    "    embed = reducer.fit_transform(X)\n",
    "    dfp = pd.DataFrame(embed, columns=['UMAP1','UMAP2'])\n",
    "    dfp['label'] = labels\n",
    "    plt.figure(figsize=(8,6))\n",
    "    sns.scatterplot(data=dfp, x='UMAP1', y='UMAP2', hue='label', palette='tab10', s=30)\n",
    "    plt.title('UMAP projection')\n",
    "    plt.legend(bbox_to_anchor=(1.05,1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2dad742",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Cluster profiling & export\n",
    "# -----------------------------\n",
    "\n",
    "def profile_and_export(df_customers, features, labels, id_col='CustomerID', out_csv='outputs/customer_clusters.csv'):\n",
    "    df = df_customers.copy()\n",
    "    df['cluster'] = labels\n",
    "    # For numeric features show cluster means\n",
    "    prof = df.groupby('cluster')[features].agg(['mean','median','count']).round(3)\n",
    "    print('Cluster profile (numeric features):')\n",
    "    print(prof)\n",
    "\n",
    "    os.makedirs(os.path.dirname(out_csv), exist_ok=True)\n",
    "    df.to_csv(out_csv, index=False)\n",
    "    print('Saved cluster assignments to', out_csv)\n",
    "    return prof\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99248c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# End-to-end runner for Mall dataset\n",
    "# -----------------------------\n",
    "\n",
    "def run_segmentation_mall():\n",
    "    df_raw, name = load_dataset('mall')\n",
    "    print('Loaded dataset:', name)\n",
    "    mall = preprocess_mall(df_raw)\n",
    "    basic_eda(mall)\n",
    "\n",
    "    # Choose features\n",
    "    # If Gender present, convert to numeric\n",
    "    proc = mall.copy()\n",
    "    if 'Gender' in proc.columns:\n",
    "        proc['Gender'] = proc['Gender'].astype(str).str.lower().map({'male':0,'female':1}).fillna(-1)\n",
    "\n",
    "    # find numeric features except CustomerID\n",
    "    features = [c for c in proc.columns if c != 'CustomerID']\n",
    "    X = proc[features].select_dtypes(include=[np.number]).fillna(0)\n",
    "\n",
    "    Xs, scaler = scale_features(X)\n",
    "\n",
    "    # Determine k\n",
    "    dfk = find_best_k(Xs, k_min=2, k_max=8)\n",
    "    print(dfk)\n",
    "    plot_elbow_silhouette(dfk)\n",
    "\n",
    "    # pick k (you can change)\n",
    "    k = int(dfk.sort_values('silhouette', ascending=False).iloc[0]['k'])\n",
    "    print('Choosing k =', k)\n",
    "    km, labels = run_kmeans(Xs, k=k)\n",
    "\n",
    "    plot_clusters_2d(Xs, labels, title=f'Mall Customers - KMeans k={k}')\n",
    "    prof = profile_and_export(proc[['CustomerID'] + features], features, labels)\n",
    "    joblib.dump({'scaler':scaler, 'kmeans':km, 'features':features}, 'outputs/mall_segmentation_pipeline.joblib')\n",
    "    print('Saved pipeline to outputs/mall_segmentation_pipeline.joblib')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9a9a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# End-to-end runner for Retail dataset (RFM)\n",
    "# -----------------------------\n",
    "\n",
    "def run_segmentation_retail():\n",
    "    df_raw, name = load_dataset('retail')\n",
    "    print('Loaded dataset:', name)\n",
    "    basic_eda(df_raw)\n",
    "\n",
    "    # Attempt to create TotalAmount if not present\n",
    "    if 'TotalAmount' not in df_raw.columns and 'Quantity' in df_raw.columns and 'UnitPrice' in df_raw.columns:\n",
    "        df_raw['TotalAmount'] = df_raw['Quantity'] * df_raw['UnitPrice']\n",
    "\n",
    "    rfm = compute_rfm_from_retail(df_raw, customer_id_col='CustomerID', invoice_date_col='InvoiceDate', invoice_no_col='InvoiceNo', amount_col='TotalAmount')\n",
    "    print('RFM head:')\n",
    "    print(rfm.head())\n",
    "\n",
    "    # log transform Monetary to reduce skew\n",
    "    rfm['Monetary_log'] = np.log1p(rfm['Monetary'])\n",
    "    features = ['Recency','Frequency','Monetary_log']\n",
    "    X = rfm[features]\n",
    "    Xs, scaler = scale_features(X)\n",
    "\n",
    "    # find k\n",
    "    dfk = find_best_k(Xs, k_min=2, k_max=8)\n",
    "    print(dfk)\n",
    "    plot_elbow_silhouette(dfk)\n",
    "\n",
    "    # choose best silhouette\n",
    "    k = int(dfk.sort_values('silhouette', ascending=False).iloc[0]['k'])\n",
    "    print('Choosing k =', k)\n",
    "    km, labels = run_kmeans(Xs, k=k)\n",
    "\n",
    "    plot_clusters_2d(Xs, labels, title=f'Retail RFM - KMeans k={k}')\n",
    "    prof = profile_and_export(rfm[['CustomerID'] + features], features, labels, id_col='CustomerID', out_csv='outputs/retail_customer_clusters.csv')\n",
    "    joblib.dump({'scaler':scaler, 'kmeans':km, 'features':features}, 'outputs/retail_segmentation_pipeline.joblib')\n",
    "    print('Saved pipeline to outputs/retail_segmentation_pipeline.joblib')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac52e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# Simple ETL wrapper to recompute clusters (callable from CLI)\n",
    "# -----------------------------\n",
    "\n",
    "def run_pipeline(dataset='mall'):\n",
    "    if dataset == 'mall':\n",
    "        run_segmentation_mall()\n",
    "    elif dataset == 'retail':\n",
    "        run_segmentation_retail()\n",
    "    else:\n",
    "        raise ValueError('dataset must be mall or retail')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2cbd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# -----------------------------\n",
    "# If executed as script, try to run both if data present\n",
    "# -----------------------------\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        # try retail first, then mall\n",
    "        if os.path.exists('data/online_retail.csv'):\n",
    "            run_pipeline('retail')\n",
    "        elif os.path.exists('data/mall_customers.csv'):\n",
    "            run_pipeline('mall')\n",
    "        else:\n",
    "            print('No dataset found in data/. Put mall_customers.csv or online_retail.csv there and re-run.')\n",
    "    except Exception as e:\n",
    "        print('Error during run:', e)\n",
    "        raise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "veh_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
